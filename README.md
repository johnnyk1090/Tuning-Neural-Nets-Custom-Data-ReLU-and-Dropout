<h1 align="center" id="title">Weight Initialization &amp; Dropout with Relu Activation</h1>

<p id="description">The project will train the model after the application of weight initialization and a specific value of Dropout on customized dataset. Relu will activate the Layers.</p>

  
  
<h2>üßê Features</h2>

Here're some of the project's best features:

*   Weight Initialization method (HE) (a model sometimes does not function properly because the weights are not initialized during the training phase )
*   Dropout (method to prevent overfitting)
*   Relu Activation function (transforming the summed weighted input from the node into the activation of the node or output for that input.)

<h2>üõ†Ô∏è Installation Steps:</h2>

<p>1. Download spec-file.txt</p>

<p>2. Follow the instructions into Conda_Virtual_Env.txt</p>

  
  
<h2>üíª Built with</h2>

Technologies used in the project:

*   Python
*   Pytorch
*   Matplotlib
*   Torchvision
